{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4c19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-04-18 15:08:19 mixins:196] _setup_tokenizer: detected an aggregate tokenizer\n",
      "[NeMo I 2025-04-18 15:08:19 mixins:330] Tokenizer SentencePieceTokenizer initialized with 32 tokens\n",
      "[NeMo I 2025-04-18 15:08:19 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-04-18 15:08:19 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-04-18 15:08:19 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-04-18 15:08:19 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-04-18 15:08:19 aggregate_tokenizer:72] Aggregate vocab size: 4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-04-18 15:08:20 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    batch_size: null\n",
      "    num_workers: 8\n",
      "    use_lhotse: true\n",
      "    max_duration: 40\n",
      "    pin_memory: true\n",
      "    use_bucketing: false\n",
      "    bucket_duration_bins: null\n",
      "    num_buckets: 1\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    batch_duration: 360\n",
      "    quadratic_duration: 15\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-04-18 15:08:20 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 8\n",
      "    shuffle: false\n",
      "    num_workers: 0\n",
      "    pin_memory: true\n",
      "    tarred_audio_filepaths: null\n",
      "    use_lhotse: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: false\n",
      "    \n",
      "[NeMo W 2025-04-18 15:08:20 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 0\n",
      "    pin_memory: true\n",
      "    tarred_audio_filepaths: null\n",
      "    use_lhotse: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-04-18 15:08:20 features:289] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-04-18 15:08:29 nemo_logging:349] /home/chaseez/.conda/envs/asl-segmentation/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-04-18 15:08:47 save_restore_connector:249] Model EncDecMultiTaskModel was successfully restored from /home/chaseez/.cache/huggingface/hub/models--nvidia--canary-1b/snapshots/51d6c4d5d4c20250a1f06f3b83e50241cfabca35/canary-1b.nemo.\n",
      "138\n",
      "loading audio...\n",
      "Getting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-04-18 15:08:49 nemo_logging:349] /home/chaseez/.conda/envs/asl-segmentation/lib/python3.10/site-packages/nemo/collections/asr/parts/preprocessing/features.py:417: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "      with torch.cuda.amp.autocast(enabled=False):\n",
      "    \n",
      "[NeMo W 2025-04-18 15:09:12 nemo_logging:349] /home/chaseez/.conda/envs/asl-segmentation/lib/python3.10/site-packages/torch/nn/modules/conv.py:549: UserWarning: cuDNN cannot be used for large non-batch-splittable convolutions if the V8 API is not enabled or before cuDNN version 9.3+. Consider upgrading cuDNN and/or enabling the V8 API for better efficiency. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647382455/work/aten/src/ATen/native/Convolution.cpp:430.)\n",
      "      return F.conv2d(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from argparse import ArgumentParser\n",
    "from canary import CanaryEncoder\n",
    "from kneed import KneeLocator\n",
    "from torch.optim import AdamW\n",
    "from whisper import Whisper\n",
    "from pathlib import Path\n",
    "from optim import FISTA\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "\n",
    "def shrinkage_operator(u, tresh):\n",
    "        return torch.sign(u) * torch.maximum(torch.abs(u) - tresh, torch.tensor(0.0, device=u.device))\n",
    "\n",
    "def project(u):\n",
    "    u = u / torch.norm(u, p=2)\n",
    "    # Test it between -1 and 1. Maybe 0 and 1 are better for mathematical properties??\n",
    "    return torch.clamp(u, 0, 1)\n",
    "\n",
    "def jitter_loss(A):\n",
    "    return 0\n",
    "\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('-e', '--encoder', type=str, default='whisper', help='Specify which encoder: Whisper or Canary')\n",
    "# parser.add_argument('-n', '--epochs', type=int, default=10000, help='Specify how many epochs.')\n",
    "# argv = parser.parse_args()\n",
    "\n",
    "\n",
    "encoder = 'canary'\n",
    "steps = 5\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model = None\n",
    "dim_embd = None\n",
    "if 'can' in encoder:\n",
    "    model = CanaryEncoder(device).to(device)\n",
    "    dim_embd = 1024\n",
    "else:\n",
    "    model = Whisper().to(device)\n",
    "    dim_embd = 1280\n",
    "\n",
    "\n",
    "# Found on page 3 at the end of the page\n",
    "activation_weight = 0.2424\n",
    "embd_basis_weight = 0.3366\n",
    "jitter_loss_weight = 0.06\n",
    "\n",
    "# I DUNNO WHAT THESE DO\n",
    "embd_lagrange_multiplier = 2\n",
    "activation_lagrange_multiplier = 2\n",
    "\n",
    "files = list(Path('../data/amicorpus').rglob('*Mix-Headset.wav'))\n",
    "\n",
    "print(len(files))\n",
    "\n",
    "for file in files:\n",
    "    print('loading audio...', flush=True)\n",
    "    waveform, sample_rate = torchaudio.load(file)\n",
    "    waveform = waveform.to(device)[:16000]\n",
    "\n",
    "    embeddings = None\n",
    "\n",
    "    tot_len = waveform.shape[-1]\n",
    "    segments = math.floor(tot_len / (sample_rate * 6))\n",
    "\n",
    "    # put files in 3 second windows\n",
    "    clipped_wav = waveform[:,:segments * sample_rate * 6]\n",
    "\n",
    "    print('Getting embeddings...', flush=True)\n",
    "    if isinstance(model, Whisper):\n",
    "        clipped_wav = clipped_wav.reshape(-1, sample_rate * 3)\n",
    "\n",
    "        # biggest batch size an A100 can handle (3s * 1024 / 60 = 51.2 minutes of audio)\n",
    "        loader = DataLoader(clipped_wav, batch_size=1024)\n",
    "        embeddings = []\n",
    "        for wav in loader:\n",
    "            embeddings.append(model(wav, sample_rate).last_hidden_state)\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "    else:\n",
    "        clipped_signal_length = torch.full((clipped_wav.shape[0],), clipped_wav.shape[1], device=device).contiguous()\n",
    "        embeddings = model(clipped_wav, clipped_signal_length)\n",
    "\n",
    "        # Rearrange shape from (batch, 1024, T) => (batch, T, 1024)\n",
    "        embeddings = embeddings.permute(0,2,1).contiguous()\n",
    "\n",
    "    # flatten the shape to be (T, model_dim)\n",
    "    embeddings = embeddings.flatten(start_dim=0, end_dim=1)\n",
    "    embd = embeddings[::100,:].contiguous().cpu().to(torch.float32)\n",
    "\n",
    "    # SVD is used to calculate the number of speakers\n",
    "    print('calculating SVD...', flush=True)\n",
    "    _, s, _ = np.linalg.svd(embd)\n",
    "\n",
    "    print('Generating Knee', flush=True)\n",
    "    knee = KneeLocator(np.arange(s.shape[0]), s, S=1.0, curve='concave', direction='decreasing')\n",
    "    \n",
    "    num_speakers = knee.knee * 2\n",
    "\n",
    "    embd_basis_matrix = torch.nn.init.kaiming_normal(torch.randn((dim_embd, num_speakers))).to(device)\n",
    "    activation_matrix = None\n",
    "\n",
    "    if isinstance(model, Whisper):\n",
    "        # (k, T)\n",
    "        activation_matrix = torch.nn.init.kaiming_normal(torch.randn((num_speakers, 2 * 1500))).to(device)\n",
    "    else:\n",
    "        activation_matrix = torch.nn.init.kaiming_normal(torch.randn((num_speakers, embeddings.shape[1]))).to(device)\n",
    "\n",
    "    embd_basis_matrix.requires_grad_()\n",
    "    activation_matrix.requires_grad_()\n",
    "\n",
    "    embd_optim = AdamW([embd_basis_matrix], lr=0.001)\n",
    "    activation_optim = AdamW([activation_matrix], lr=0.001)\n",
    "\n",
    "    batch_size = None\n",
    "    if isinstance(model, Whisper):\n",
    "        # (batch, 1500, 1280) for 3 second window\n",
    "        batch_size = 2 * 1500\n",
    "    else:\n",
    "        # (batch, 1024, T) => (batch, T, 1024) for 6 second window\n",
    "        batch_size = embeddings.shape[1]\n",
    "\n",
    "    loader = DataLoader(embeddings, batch_size=batch_size)\n",
    "\n",
    "    print('Starting Training', flush=True)\n",
    "    for step in range(steps):\n",
    "        # CANNOT BE BATCHED YET \n",
    "        for embd in tqdm(loader):\n",
    "            print(embd.size, flush=True)\n",
    "            # Reshape embd to be MxT instead of TxM\n",
    "            y_hat = embd.T - (embd_basis_matrix @ activation_matrix.detach())\n",
    "            term1 = torch.norm(y_hat)\n",
    "            term2 = embd_basis_weight * torch.norm(embd_basis_matrix)\n",
    "            term3 = activation_weight * torch.norm(activation_matrix.detach())\n",
    "            term4 = jitter_loss_weight * jitter_loss(activation_matrix.detach())\n",
    "\n",
    "            loss = term1 + term2 + term3 + term4\n",
    "\n",
    "            loss.backward()\n",
    "            embd_optim.step()\n",
    "            embd_optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embd_basis_matrix = project(shrinkage_operator(embd_basis_matrix, embd_lagrange_multiplier))\n",
    "\n",
    "            y_hat = embd - (embd_basis_matrix.detach() @ activation_matrix)\n",
    "            term1 = torch.norm(y_hat)\n",
    "            term2 = embd_basis_weight * torch.norm(embd_basis_matrix.detach())\n",
    "            term3 = activation_weight * torch.norm(activation_matrix)\n",
    "            term4 = jitter_loss_weight * jitter_loss(activation_matrix)\n",
    "\n",
    "            loss = term1 + term2 + term3 + term4\n",
    "\n",
    "            loss.backward()\n",
    "            activation_optim.step()\n",
    "            activation_optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                activation_matrix = project(shrinkage_operator(activation_matrix, activation_lagrange_multiplier))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032f98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
